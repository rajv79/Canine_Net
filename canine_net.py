# -*- coding: utf-8 -*-
"""Canine Net

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DMsecoPKW1xxZmDDdZXCDWIFXdd98Ewn

### Identification Dog Breed
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import skimage
import sklearn
from google.colab import drive
import csv

##Importing Tensorflow and checking its version
import tensorflow as tf
import tensorflow_hub as tfhub
print("The version of Tensorflow:",tf.__version__)
print("The version of Tensorflow_hub:",tfhub.__version__)
## checking if the GPU is availabe
print("GPU," "availabe(yesss)" if tf.config.list_physical_devices("GPU") else "not availabel" )

drive.mount('/content/drive')
training_object = open("/content/drive/MyDrive/Neural_Network_Dataset/Datasets/labels.csv")
df = pd.read_csv(training_object)

"""### Loading our Data and check if it correct or not"""

df.head(15)  ## checkig the data frame

df.shape, len(df), df.describe  ## checking the info about the our dataframe

## checking the unqiue names in the breed cl
len(np.unique(df["breed"])) ## so we can see that we have 120 different breed
np.unique(df["breed"])

"""### How many images are there of each breed"""

df["breed"].value_counts()  ## so this will how many iamges are there for each breed type

### Lets plot the graph of all the breeds
df["breed"].value_counts().plot.bar(figsize=(20,20))

## checking the median and mean value of breed columns
df["breed"].value_counts().mean()

df["breed"].value_counts().median()

"""### Viewing an Image"""

from IPython.display import Image
Image("/content/drive/MyDrive/Neural_Network_Dataset/Datasets/train/000bec180eb18c7604dcecc8fe0dba07.jpg")

from IPython.display import Image
Image("/content/drive/MyDrive/Neural_Network_Dataset/Datasets/train/ffd3f636f7f379c51ba3648a9ff8254f.jpg")

"""### Getting Images and their labels for X AND Y


"""

### create pathname form iImage ID'S
### The below filename_list will work as X labels for our data training data set
filename_list = ["/content/drive/MyDrive/Neural_Network_Dataset/Datasets/train/"+ fname + ".jpg"for fname in df["id"]]


## checking the id of all images
filename_list[:10]

import os
os.listdir("/content/drive/MyDrive/Neural_Network_Dataset/Datasets/train")[:10]

"""Checking for Missing"""

df.isna().sum()

## Checking whether number of filenames match number of actual images file

import os
if len(os.listdir("/content/drive/MyDrive/Neural_Network_Dataset/Datasets/train"))==len(filename_list):
  print("Filename match")
else:
  print("Filename do no match actual amount")

df[:10]

## if you want to check some name of breed
df["breed"][1]

"""## Preparing our labels"""

# we make our labels into an array form

labels = df["breed"].to_numpy()  ## this will make array of all the dog breed

labels

## See if number of labels matches filename

if(len(labels)== len(filename_list)):
  print("Match !!! there is no missing data")
else:
  print("missing data!!! please drop those")

print(labels[0])

"""### Turing all labels into boolean array and then convert into numbers"""

unique_labels = np.unique(df["breed"])

from pandas.core.arrays import boolean
boolean_labels = [labels == unique_labels for labels in labels]

## checking the len of boolean_labels
len(boolean_labels)

boolean_labels[:10]

"""### Converting our boolean_labels into integers"""

## example: Turing boolean array into integers
print(labels[0])  ## original labels
print(np.where(unique_labels==labels[0]))  # index where labels occur
print(boolean_labels[0].argmax) # idex where labels occur in bool
print(boolean_labels[0].astype(int))  ### This will convert the idex into array formate of 0's and 1's

"""## Spilt the data into X AND Y"""

X = filename_list   ## this is like features for image
Y = boolean_labels  ## this is labels or target

"""As we know there are 10,000 data and experimenting with them will take to much time, so we will just used some amount of training data in starting

## Split the data into Train and validation
"""

## For now we will take 1000 training data
NUM_IMAGES = 1000

## Now we use use train_test split lib, to make the data into training and validation set, instead of test, because we already have test data set

#lets split our data into tra and validataion
from sklearn.model_selection import train_test_split
X_train, X_val, Y_train, Y_val = train_test_split(X [:NUM_IMAGES],Y[:NUM_IMAGES],test_size =0.2)

### Checking the len of X_train, X_val Y_train, Y_val
len(X_train),len(X_val) , len(Y_train), len(Y_val)

"""## Preprocessing Images(Turing images into Tensoors)

To preprocess our images into tensor we are going to write a function

1. Take an image filepath as input
2. Use Tensorflow to read file and save it to variable, image
3. Turn images (a.jpg) in tensor
4. Normalize our image (convert color channel from 0-255 to 0-1)
4. Resize the image to be a shape (224,224)
5. retrun the modified image

Converting the images into array
"""

### convert the images to Numpy
from matplotlib.pyplot import imread
# "/content/drive/MyDrive/Neural_Network_Dataset/Datasets/train/000bec180eb18c7604dcecc8fe0dba07.jpg"
image = imread(filename_list[42])
image.shape

image.max(), image.min()
## Images are made up of RGB, which ranges from 0 to 255 pixel value

### changing the image to Tensor
tf.constant(image)[:2]

"""## Create a function to convert image into tensor"""

# Define image Size
IMG_SIZE = 224

# CREATE a function for preprocessing image

def process_images(image_file_path):

  """
  Taking the image file path and turns the image into tensor
  """

  # Reaing an image file
  image = tf.io.read_file(image_file_path)
  ## Turn the jpeg image into numerical Tensor with 3 color channels
  image = tf.image.decode_jpeg(image, channels=3)

  ## convert the colour channel value from 0-255 to 0-1 values(Normalization)
  image = tf.image.convert_image_dtype(image,tf.float32)

  ## Resize the images to our desired value (224,224)

  image = tf.image.resize(image, size=[IMG_SIZE,IMG_SIZE])

  return image

#tensor = tf.io.read_file(filename_list[26])
#tensor

"""### Turning Data into Batches"""

### The batch size should not be more than 32
### I order to use Tensorflow effective ,we need our data in the form of Tensor tuples( which look like (image, label))

"""Creating a function to return tuple for image"""

def get_image_label(image_path, label):
  image= process_images(image_path)
  return image, label

### testing the above function
(process_images(X[42]),tf.constant(Y[42]))   # where X = filepaths names as features and Y = targert(breed labels)

"""Now we will make a function to convert all of our data X and Y into batches"""

## Define the batch size , 32

BATCH_SIZE =32

### Creating a function to turn data into batches

def create_data_batches(X, Y=None, batch_size=BATCH_SIZE, valid_data = False, test_data= False):

  """
   creates batch of data out of image (x) and (y) pairs
   Shuffles the data if it's training data but doesn't shuffle if its validation data
   Also accept test data as input(No labels)

  """

  # If the data is test data, we probably dont have labesl

  if test_data  :
    print("Creating the test data batches !")
    data = tf.data.Dataset.from_tensor_slices((tf.constant(X)))  # only file path , with no labels
    data_batch = data.map(process_images).batch(BATCH_SIZE)
    return data_batch



  # If the data is valid data set, we will not shuffle
  elif valid_data:
    print("Creating the  valdation data batches !")
    data = tf.data.Dataset.from_tensor_slices((tf.constant(X),tf.constant(Y)))  # only file path= X and labels =Y
    data_batch = data.map(get_image_label).batch(BATCH_SIZE)
    return data_batch

  ## IF the data is training data set  we will shuffer

  else:
    print("Creating the trainning data batches !")
    data = tf.data.Dataset.from_tensor_slices((tf.constant(X), tf.constant(Y))) # only file path , with labels

    ## shuffle pathnames and labels before images process to save time
    #data = data.shuffle(buffer_size = len(X))   # we will shuffle whole data set

    data = data.map(get_image_label)   # create (image,label) to tuples

    data_batch = data.batch(BATCH_SIZE)

  return data_batch

"""### Creating traning and Validation data set"""

train_data = create_data_batches(X_train, Y_train)
val_data = create_data_batches(X_val, Y_val, valid_data = True)

## Check out the different attributes of our data batches
train_data.element_spec , val_data.element_spec

"""### Visualizing the Images of batch"""

#train_data  ## just checking the training data set form the batches

def visual_image(image, labels):
  plt.figure(figsize=(15,15))

  for i in range(25):
    ax= plt.subplot(5,5,i+1)  #( we will have 5 rows and 5 columns)
    plt.imshow(image[i])      # display the images

    ## adding the images labels as title which is breed name
    plt.title(unique_labels[labels[i].argmax()])
    plt.axis("off")

## the below code is use to loop through the batches on after the other, and undo the batches and put into a train image and train label
train_image, train_labels = next(train_data.as_numpy_iterator())

## Now we can see the images from the training data sets in batches

visual_image(train_image, train_labels)

"""### Building a Models"""

# Setup the input shape to the model
INPUT_SHAPE = [None,IMG_SIZE,IMG_SIZE, 3]  # NONE = BATCH =32, IMAGE_SIZE =224, 3 = rgb(Channel)

# setup the output the shape of our model
OUTPUT_SHAPE = len(unique_labels)   # the output will be one of the unique labels of breeds=120
## setting the MobileNet V2 models from tf hub
MODEL_URL = "https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/5"

"""### Creating function for model"""

def create_model( input_shape =INPUT_SHAPE,output_shap= OUTPUT_SHAPE,model_url=MODEL_URL ):


  ## setup th model layers, where units = 120 ,becouse we need only 120 breeds ,insted of mobilnet v2 units
  model = tf.keras.Sequential([
      tfhub.KerasLayer(MODEL_URL), ## layer=1 , input layer
      tf.keras.layers.Dense(units= OUTPUT_SHAPE,activation= "softmax") ## layer 2 (output layer)

  ])

  ## compile the model

  model.compile(
      loss = tf.keras.losses.CategoricalCrossentropy(), ## minimize the loss (more the loss worst the model is trianing and gusseing)
      optimizer = tf.keras.optimizers.Adam(),  ## help to reduce the loss function
      metrics = ["accuracy"]
  )

  ## Build the model
  model.build(INPUT_SHAPE)

  return model

model = create_model() ## creating the model
model.summary()

## To see your model weights
#model.weights

## This line of code help you to graph the layer of our model and shape of input and output
tf.keras.utils.plot_model(model,show_shapes=True)

"""### Creating  TensorBaord callbacks
This will help to see how our model is doing during traning

"""

from keras.callbacks import TensorBoard
## TensorBoard callback function
#%load_ext TensorBoard

import  datetime
## create a function to build a tensorflow callback

def create_tensorboard_callback():

  # create a log directory for storing tensorboard
    logdir = os.path.join("/content/drive/MyDrive/Neural_Network_Dataset/Datasets/logs",
                          datetime.datetime.now().strftime("%Y%m%d -%H%M%S"))

    return tf.keras.callbacks.TensorBoard(logdir)

"""### Creating Early Stopping Callback function"""

Early_stop_callback =tf.keras.callbacks.EarlyStopping(monitor="val_accuracy",patience=3)

"""### Trainning a model"""

# define the number of epocs
NUM_EPOCS = 100


def train_model():

  # Train a give model and retrun the train model

  # creating the model
  model = create_model()
  #Create a tensorboard when we training model
  tenserboard = create_tensorboard_callback()

  # fitting the model
  model.fit(train_data, epochs=NUM_EPOCS,validation_data=val_data,validation_freq=1, callbacks=[tenserboard, Early_stop_callback])

  return model

# Fit the model to the data
model = train_model()

"""### Making and evaluting prediction using a trained model"""

val_data

# The prediction will contain probabiliies and labels
predictions= model.predict(val_data,verbose=1)
predictions

#np.sum(predictions[0])

#First prediction
index =90
print(predictions[index])
print(f"Max value(probablity): {np.max(predictions[index])}")
print(f"Sum:{np.sum(predictions[index])} ")
print(f"Max index: {np.argmax(predictions[index])}")
print(f"Predicted label:  {unique_labels[np.argmax(predictions[index])]}")

#unique_labels[119]

## Turn prediction probs into their labels of breeds

def get_breed_labels(prediction_probabilites):
  return unique_labels[np.argmax(prediction_probabilites)]

## checking the labels
#pred_label = get_breed_labels(predictions[81])
#pred_label

"""### --------------------------------------Function for  unbatch the dataset ------------------------------------------"""

## Define a function that will unbatch from the from the validation data set and store images and labels in the list for predictions
def unbatch_data(validation_data_Set):

  # creating list to store image and there labels
  images =[]
  labels =[]

  for image ,label in validation_data_Set.unbatch().as_numpy_iterator():
    images.append(image)
    labels.append(unique_labels[np.argmax(label)])
  return images, labels

## Checking the preidction
val_images, val_labels = unbatch_data(val_data)

"""### Plotting the images of  predicted label vs actual label"""

def plot_Preds_data(prediction_probabilites, labels, images, n=1):
  pred_prob, true_label, image = prediction_probabilites[n],labels[n], images[n]

  # pred_label
  pred_label = get_breed_labels(pred_prob)

  #plotting the graph
  plt.imshow(image)
  plt.xticks([])
  plt.yticks([])


  # change the color of title if the prediction is wrong or correct
  if pred_label == true_label:
    color = "green"
  else:
    color= "red"

  plt.title("{} {:2.0f}% {}".format(pred_label,
                                    np.max(pred_prob)*100,
                                    true_label),
                                    color =color)

"""## Plotting the image,label accuracy of breeds"""

next =0
rows =3
cols=3
num_images = rows*cols

plt.figure(figsize=(20,20))

for i in range(num_images):
  plt.subplot(rows,1*cols,1*i+1)

  # this functon will plot the images of our labels and predicted labels
  plot_Preds_data(predictions,
          labels= val_labels,
          images = val_images,n=i+next)

model.evaluate(val_data)

"""### -----------------------------------Saving and loading function ---------------------------"""

#create a function to save a model
def save_model(model, suffix= None):
  modeldir = os.path.join ("/content/drive/MyDrive/Neural_Network_Dataset/models")
  model_Path = modeldir + "_" +suffix +".h5"  # save the formate of model
  model.save(model_Path)
  return model_Path

## loading the model

def load_model(model_path):

  print(f"Loading the model: {model_path}")
  model = tf.keras.models.load_model(model_path, custom_objects={"KerasLayer" :tfhub.KerasLayer})
  return model

#save_model(model,suffix="1000 images-on mobilnetv2")

## code for loading the data back:
#loaded_image_model = load_model("/content/drive/MyDrive/Neural_Network_Dataset/models_1000 images-on mobilnetv2.h5")

"""### ------------------------------Final-Training on 10,000 Images on mobilenetv2 model---------------------------------------

"""

# we created the batch of full data set by calling our function
full_training_data = create_data_batches(X,Y)

## Now we will create a model from create model data set
model_mobilNet = create_model()  ## This is our model name

## create full model early stopping
full_training_data_early_stopping = tf.keras.callbacks.EarlyStopping(monitor = "accuracy",
                                                                              patience =3)

## Fit the model
model_mobilNet.fit(full_training_data,epochs=NUM_EPOCS, callbacks =[full_training_data_early_stopping])

"""###           -------------------------------Saving our Final Model----------------------------"""

save_model(model_mobilNet,suffix="Trained with 10k images_of dog breeds") # saving our full model that is trained with 10k images

"""Loading our Fully Trained New model

### ---------------------Loading our Model -------------------------------------
"""

#loading our Model_mobilnet
load_model_mobilNet = load_model("/content/drive/MyDrive/Neural_Network_Dataset/models_Trained with 10k images_of dog breeds.h5")

"""### ----------------------------------Making Prediction on the Test Dataset-------------------------------------
Since model is been trained on images in form of Tensor batch, to make prediction on the test data, we have to get into same formate
"""

### load test images
test_image_path = "/content/drive/MyDrive/Neural_Network_Dataset/Datasets/test/"
test_filenamees = [test_image_path +fname for fname in os.listdir(test_image_path)]

#test_filenamees[:10]

len(test_filenamees)

## Create test data batch by calling create_Data_batch function
test_Data_images = create_data_batches(test_filenamees, test_data = True)

test_Data_images

## Make prediction on test data batch using the loaded full model
test_prediction = load_model_mobilNet.predict(test_Data_images,verbose =1)

## Save prediction (Numpy array)
#np.savetxt("/content/drive/MyDrive/Neural_Network_Dataset/Datasets/preds_Array.csv",test_prediction,delimiter = ",")

##loading our prediction
#test_predictions_prob = np.loadtxt("/content/drive/MyDrive/Neural_Network_Dataset/Datasets/preds_Array.csv",delimiter = ",")

#test_predictions_prob.shape



"""### ---------------------------------------------Making prediction on Custom Dog Image----------------------------------------"""

custome_file_image = "/content/drive/MyDrive/Neural_Network_Dataset/Datasets/Custom_Image/"
Custom_filenamees = [custome_file_image +fname for fname in os.listdir()]

len(Custom_filenamees)

## Now pass our custom file names into batch function

custom_Dog_Data = create_data_batches(Custom_filenamees,test_data=True)

custom_Dog_Data

Custom_prediction = load_model_mobilNet.predict(custom_Dog_Data,verbose=1)

Custom_prediction.shape

## Get Custom Image prediction label
Custom_pred_labels = [get_breed_labels(Custom_prediction[i]) for i in range (len(Custom_prediction))]
Custom_pred_labels

custom_image = []

for image in custom_Dog_Data.unbatch().as_numpy_iterator():
  custom_image.append(image)

## Check Custom image prediction

plt.figure(figsize=(25,25))
for i, image in enumerate(custom_image):
  plt.subplot(1,4,i+1)
  plt.xticks([])
  plt.yticks([])
  plt.title(Custom_pred_labels[i])
  plt.imshow(image)

